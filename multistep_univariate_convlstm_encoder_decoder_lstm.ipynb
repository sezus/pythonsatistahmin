{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sezgi.sener\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\sezgi.sener\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 1)\n",
      "lstm: [45.108] 14.5, 46.9, 37.1, 33.6\n",
      "45.10769042360252 [14.494435920478217, 46.85155129100309, 37.14068625701276, 33.62861532213623]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 4, 1)\n",
      "lstm: [34.443] 9.5, 31.3, 26.7, 21.6\n",
      "34.44305053953192 [9.50879090534888, 31.349777820283848, 26.716489722347347, 21.645017554958063]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4, 1)\n",
      "lstm: [39.395] 12.0, 37.2, 31.9, 28.7\n",
      "39.39516782610369 [12.049004890015373, 37.20530296794284, 31.874851101447724, 28.690367575016573]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 4, 1)\n",
      "lstm: [36.847] 14.3, 32.4, 31.7, 26.1\n",
      "36.847339665567276 [14.253979780641002, 32.37087099695298, 31.693747485246295, 26.105657651154544]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1)\n",
      "lstm: [38.657] 17.3, 34.1, 31.2, 27.6\n",
      "38.65680577611787 [17.30654329852639, 34.07803675307418, 31.249389596136535, 27.616928820114357]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 4, 1)\n",
      "lstm: [35.533] 13.5, 32.5, 27.8, 26.8\n",
      "35.53311203344354 [13.548216401974553, 32.46950667151659, 27.839304477891957, 26.831696611762496]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0          1          2          3\n",
      "0  14.494436  46.851551  37.140686  33.628615\n",
      "1   9.508791  31.349778  26.716490  21.645018\n",
      "2  12.049005  37.205303  31.874851  28.690368\n",
      "3  14.253980  32.370871  31.693747  26.105658\n",
      "4  17.306543  34.078037  31.249390  27.616929\n",
      "5  13.548216  32.469507  27.839304  26.831697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# univariate multi-step encoder-decoder convlstm for the power usage dataset\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from matplotlib import pyplot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data,n):\n",
    "\t# split into standard weeks\n",
    "\ttrain, test = data[0:-n], data[-n:]    \n",
    "\t# restructure into windows of weekly data\n",
    "\t#train = array(split(train, len(train)/7))\n",
    "\t#test = array(split(test, len(test)/7))\n",
    "\ttrain = array(split(train, len(train)/4))\n",
    "\ttest = array(split(test, len(test)/4))    \n",
    "\treturn train, test\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "\tscores = list()\n",
    "\tmae_scores = list()      \n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\t# calculate mse\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\t# calculate rmse\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\tmae = mean_absolute_error(actual[:, i], predicted[:, i])            \n",
    "\t\t# store\n",
    "\t\tscores.append(mae)\n",
    "\t\tmae_scores.append(mae)        \n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\trmse_=DataFrame(scores)           \n",
    "\tmae_=DataFrame(mae_scores)    \n",
    "\tsonuc=pd.concat([rmse_, mae_], axis=1, ignore_index=True)        \n",
    "\texport_sonuc= sonuc.to_excel (r'C:\\Users\\sezgi.sener\\Desktop\\New folder\\deep_learning_time_series_forecasting\\code\\MULTISTEP\\LSTM\\encoder_decoder_lstm_sonuc.xlsx', index = None, header=True)        \n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores\n",
    "\n",
    "# summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=4):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\t# step over the entire history one time step at a time\n",
    "\tfor _ in range(len(data)):\n",
    "\t\t# define the end of the input sequence\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\t# ensure we have enough data for this instance\n",
    "\t\tif out_end < len(data):\n",
    "\t\t\tx_input = data[in_start:in_end, 0]\n",
    "\t\t\tx_input = x_input.reshape((len(x_input), 1))\n",
    "\t\t\tX.append(x_input)\n",
    "\t\t\ty.append(data[in_end:out_end, 0])\n",
    "\t\t# move along one time step\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# train the model\n",
    "def build_model(train,test, n_steps, n_length, n_input):\n",
    "\t# prepare data\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\ttest_x, test_y = to_supervised(test, n_input)  \n",
    "\t# define parameters\n",
    "\tverbose, epochs, batch_size = 0, 20, 16\n",
    "\tn_features, n_outputs = train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape into subsequences [samples, timesteps, rows, cols, channels]\n",
    "\ttrain_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
    "\ttest_x = test_x.reshape((test_x.shape[0], n_steps, 1, n_length, n_features))    \n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\ttest_y = test_y.reshape((test_y.shape[0], test_y.shape[1], 1))    \n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\tplot_model(model, show_shapes=True, to_file='convlstm_encoder_decoder_lstm.png')     \n",
    "\tinitial_weights = model.get_weights()    \n",
    "\t# fit network\n",
    "\trepeats=1\n",
    "\tfor i in range(repeats):\n",
    "\t\t#model.set_weights(initial_weights)      \n",
    "\t\tmodel.compile(loss='mse', optimizer='adam', metrics=['mse','accuracy','mape'])          \n",
    "\t\taa=model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,validation_data=(test_x, test_y), verbose=verbose) \n",
    "\t\tpyplot.plot(aa.history['loss'],  color='blue')\n",
    "\t\tpyplot.plot(aa.history['val_loss'], color='orange')\n",
    "\t\t#print('%d) TrainRMSE=%f, TestRMSE=%f' % (i, xx.history['loss'].iloc[-1], xx.history['val_loss'].iloc[-1]))\n",
    "\t\tmodel.reset_states() \n",
    "\t\t#reset_weights(model)         \n",
    "\tpyplot.savefig('epochs_diagnostic_convlstm_lstm.png')   \n",
    "\tpyplot.legend()\n",
    "\tpyplot.show()     \n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model\n",
    "\n",
    "# make a forecast\n",
    "def forecast(model, history, n_steps, n_length, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, 0]\n",
    "\t# reshape into [samples, timesteps, rows, cols, channels]\n",
    "\tinput_x = input_x.reshape((1, n_steps, 1, n_length, 1))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=0)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_steps, n_length, n_input):\n",
    "\t# fit model\n",
    "\tmodel = build_model(train,test, n_steps, n_length, n_input)\n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "\t# walk-forward validation over each week\n",
    "\tpredictions = list()\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# predict the week\n",
    "\t\tyhat_sequence = forecast(model, history, n_steps, n_length, n_input)\n",
    "\t\t# store the predictions\n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\t# get real observation and add to history for predicting the next week\n",
    "\t\thistory.append(test[i, :])\n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\tprint(predictions.shape) \n",
    "\tpred=pd.DataFrame(predictions.reshape(predictions.shape[0],predictions.shape[1]))   \n",
    "\ttest_=pd.DataFrame(test[:,:,0])    \n",
    "\tpred=pd.concat([pred, test_], axis=1, ignore_index=True)\n",
    "\texport_pred = pred.to_excel (r'C:\\Users\\sezgi.sener\\Desktop\\New folder\\deep_learning_time_series_forecasting\\code\\MULTISTEP\\LSTM\\convlstm_encoder_lstm_predictions.xlsx', index = None, header=True)\n",
    "\tscore, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "\treturn score, scores\n",
    "\n",
    "# load the new file\n",
    "dataset = read_csv('MyData_weeks.csv', header=0, infer_datetime_format=True, \n",
    "                   parse_dates=['Weeks'], index_col=['Weeks'])# split into train and test\n",
    "# split into train and test\n",
    "son = list()\n",
    "for n in [8,9,10,11,12,13]:\n",
    "\t#print(n)    \n",
    "\ttrain, test = split_dataset(dataset.values,n*4)\n",
    "\t# define the names and functions for the models we wish to evaluate\n",
    "\tn_steps, n_length = 2, 4\n",
    "\t# define the total days to use as input\n",
    "\tn_input = n_length * n_steps\n",
    "\tscore, scores = evaluate_model(train, test, n_steps, n_length, n_input)\n",
    "\t# summarize scores\n",
    "\tsummarize_scores('lstm', score, scores)\n",
    "\t# evaluate each model\n",
    "\tprint(score,scores)\n",
    "\tson.append(scores)\n",
    "weeks = ['1', '2', '3', '4']\n",
    "a=0\n",
    "for i in [8,9,10,11,12,13]:\n",
    "\tpyplot.plot(weeks, DataFrame(son).T[a], marker='o', label=i*4)   \n",
    "\ta=a+1    \n",
    "pyplot.savefig('multistep_convlstm.png',show_shapes=True, show_layer_names=True) \n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "print(DataFrame(son))\n",
    "pyplot.boxplot(DataFrame(son).T)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
